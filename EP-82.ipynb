{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdmPCM2QSRSv"
      },
      "source": [
        "# **Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr7cgf73SWW_",
        "outputId": "58983af2-a575-4a31-dd5f-616e1801dce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from NLTK) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from NLTK) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from NLTK) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from NLTK) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyzBDoePSoxD"
      },
      "outputs": [],
      "source": [
        "sentence = \"I love to eat Pizza!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgGIl5adS0FN",
        "outputId": "2a8903d8-d6d0-435b-f750-4739e3002748"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HL37nmMTAsK"
      },
      "outputs": [],
      "source": [
        "tokens = word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1AzvadTTKdk",
        "outputId": "6b80675d-789d-4103-dbc9-5034f6eedffa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'love', 'to', 'eat', 'Pizza', '!']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WumerBzETpCc"
      },
      "outputs": [],
      "source": [
        "character = list(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FImVAhgsTyEa",
        "outputId": "6afef59b-3df6-4c6d-9e6d-3a8f20e702c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I',\n",
              " ' ',\n",
              " 'l',\n",
              " 'o',\n",
              " 'v',\n",
              " 'e',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'e',\n",
              " 'a',\n",
              " 't',\n",
              " ' ',\n",
              " 'P',\n",
              " 'i',\n",
              " 'z',\n",
              " 'z',\n",
              " 'a',\n",
              " '!']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wJGM58nTzHW"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQxHv8v9UCUP"
      },
      "outputs": [],
      "source": [
        "tokens_punc = wordpunct_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAqCJnviUJhv",
        "outputId": "45008587-a1a0-430b-e52e-79dc7909f9ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'love', 'to', 'eat', 'Pizza', '!']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokens_punc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDSdOGntUkR4"
      },
      "source": [
        "The main difference between wordpunct_tokenize() and word_tokenize() lies in how they handle punctuation marks.\n",
        "\n",
        "word_tokenize(): This function, as the name suggests, tokenizes the input string into individual words while ignoring punctuation marks. It treats consecutive non-whitespace characters as a single token, effectively separating words based on spaces.\n",
        "\n",
        "wordpunct_tokenize(): This function tokenizes the input string into words, but it treats punctuation marks as separate tokens. So, punctuation marks are retained as separate tokens in the output.\n",
        "\n",
        "For example, given the sentence \"I love eating pizza!\", word_tokenize() would produce ['I', 'love', 'eating', 'pizza'], whereas wordpunct_tokenize() would produce ['I', 'love', 'eating', 'pizza', '!'].\n",
        "\n",
        "In summary, word_tokenize() is more suitable for tasks where punctuation marks are not crucial for analysis, while wordpunct_tokenize() can be useful when retaining punctuation information is important, such as in some NLP tasks or text analysis scenarios."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
